{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b55bcf33-7012-403a-a292-68da73bebeaf",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/6/6f/Dauphine_logo_2019_-_Bleu.png\" style=\"width: 600px;\"/> \n",
    "</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa02b98d-1879-498b-945e-86536fad0267",
   "metadata": {},
   "source": [
    "<div align=\"center\"><span style=\"font-family:Arial Black;font-size:33px;color:darkblue\"> Master Economie Finance </span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6845ce9-d34f-4027-b7be-388f84a79e2e",
   "metadata": {},
   "source": [
    "<div align=\"center\"><span style=\"font-family:Arial Black;font-size:27px;color:darkblue\">Application Lab – Risk measures </span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eb3f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43140a7f",
   "metadata": {},
   "source": [
    "## Individual risk measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66f6fc1",
   "metadata": {},
   "source": [
    "### Value at risk(VaR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5773e939",
   "metadata": {},
   "source": [
    "#### Historical Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e32f8c",
   "metadata": {},
   "source": [
    "The VaR at the $\\alpha\\%$ level is the $\\alpha\\%$ quantile of the return distribution $(F)$:\n",
    "\n",
    "$$\n",
    "VaR_{t}(\\alpha) = F^{-1}_{r_{t}}(\\alpha)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6290985c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data import\n",
    "df_ret_asset = pd.read_csv('ret_asset.csv')\n",
    "df_ret_asset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0472ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ret_asset['Date'] = pd.to_datetime(df_ret_asset['Date'])\n",
    "df_ret_asset.set_index('Date', inplace = True)\n",
    "df_ret_asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8bb460",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "df_ret_asset.quantile(alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67b06de",
   "metadata": {},
   "source": [
    "#### Rolling window historical VaR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0c103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_var_historical(series, alpha=0.05, window=250):\n",
    "    return series.rolling(window).quantile(alpha)\n",
    "\n",
    "rolling_var = rolling_var_historical(df_ret_asset, alpha=0.05, window=250)\n",
    "rolling_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64acc332",
   "metadata": {},
   "source": [
    "#### Predicted VaR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca8c535",
   "metadata": {},
   "source": [
    "$$ \n",
    "VaR^\\alpha_{i, t+1 \\mid t} = \\sigma_{i, t+1 \\mid t} \\times q_{\\alpha,t}\n",
    "$$\n",
    "\n",
    "where $\\sigma_{i, t+1 \\mid t}$ can be calculated based on Garch and $q_{\\alpha,t} = F^{-1}_{\\alpha} \\left( { z_{i, t} } \\right)$ is the conditional empirical $\\alpha$-quantile of the standardized residuals $z_{i, t} = \\frac{r_{i, t}}{\\sigma_{i, t}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98bc550",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arch import arch_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc9ec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_var(df_ret, window = 250, alpha = 0.05):\n",
    "\n",
    "    # Create output DataFrames with same shape/index/columns\n",
    "    df_Vol = \n",
    "    df_Q05 = \n",
    "    df_VaR = \n",
    "\n",
    "    for col in df_ret.columns:\n",
    "        col_idx = df_ret.columns.get_loc(col) + 1\n",
    "        for j in range(window, len(df_ret[col])):\n",
    "            # Slice the rolling window for the current asset.\n",
    "            rolling_data = \n",
    "\n",
    "            # Fit GARCH(1,1) with Zero mean and normal distribution\n",
    "            garch_model = \n",
    "\n",
    "            # forecast(horizon=1).variance is σ^2，you MUST take the square root to get σ.\n",
    "            sigma_forecast = \n",
    "\n",
    "            # In-window conditional volatility σ_t and standardized residuals z_t\n",
    "            sigma_in = \n",
    "            z = \n",
    "\n",
    "            # Empirical alpha-quantile within window\n",
    "            q_alpha = \n",
    "\n",
    "            # Save the results\n",
    "            df_Vol.iloc[j, df_Vol.columns.get_loc(col)] = sigma_forecast\n",
    "            df_Q05.iloc[j, df_Q05.columns.get_loc(col)] = q_alpha\n",
    "            df_VaR.iloc[j, df_VaR.columns.get_loc(col)] = sigma_forecast * q_alpha\n",
    "\n",
    "            # Print the row and column we are running\n",
    "\n",
    "    return df_VaR, df_Vol, df_Q05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bef0f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VaR_asset, df_Vol_asset, df_Q05 = compute_var(df_ret_asset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1909c00",
   "metadata": {},
   "source": [
    "### VaR backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c91c18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VaR_asset_bt = df_VaR_asset.iloc[250:]\n",
    "df_VaR_asset_bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a8ce44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ret_asset_bt = df_ret_asset.iloc[250:]\n",
    "df_ret_asset_bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f194fed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dfc0bb",
   "metadata": {},
   "source": [
    "### Unconditional coverage test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49a81ab",
   "metadata": {},
   "source": [
    "Kupiec (1995) introduced a variation on the binomial test called the proportion of failures (POF) test.  \n",
    "The POF test works with the binomial distribution approach. In addition, it uses a likelihood ratio to test whether the probability of exceptions is synchronized with the probability \\(p\\) implied by the VaR confidence level.  \n",
    "If the data suggests that the probability of exceptions is different than \\(p\\), the VaR model is rejected.  \n",
    "\n",
    "The POF test statistic is\n",
    "\n",
    "$$\n",
    "LR_{POF} = -2 \\log \\left( \\frac{(1-p)^{N-x} p^x}{\\left(1-\\frac{x}{N}\\right)^{N-x} \\left(\\frac{x}{N}\\right)^x} \\right)\n",
    "$$\n",
    "\n",
    "where \\(x\\) is the number of failures, \\(N\\) the number of observations and \\(p = VaR level\\).  \n",
    "\n",
    "This statistic is asymptotically distributed as a chi-square variable with 1 degree of freedom.  \n",
    "The VaR model fails the test if this likelihood ratio exceeds a critical value.  \n",
    "The critical value depends on the test confidence level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2d695e",
   "metadata": {},
   "source": [
    "### Independence test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f085b69",
   "metadata": {},
   "source": [
    "Christoffersen (1998) proposed a test to measure whether the probability of observing an exception on a particular day depends on whether an exception occurred.  \n",
    "Unlike the unconditional probability of observing an exception, Christoffersen's test measures the dependency between consecutive days only.  \n",
    "The test statistic for independence in Christoffersen’s interval forecast (IF) approach is given by:\n",
    "\n",
    "$$\n",
    "LR_{CCI} = -2 \\log \\left( \n",
    "\\frac{(1 - \\pi)^{n00+n10} \\pi^{n01+n11}}\n",
    "{(1 - \\pi_0)^{n00} \\pi_0^{n01} (1 - \\pi_1)^{n10} \\pi_1^{n11}} \n",
    "\\right)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- \\(n00\\) = Number of periods with no failures followed by a period with no failures.  \n",
    "- \\(n10\\) = Number of periods with failures followed by a period with no failures.  \n",
    "- \\(n01\\) = Number of periods with no failures followed by a period with failures.  \n",
    "- \\(n11\\) = Number of periods with failures followed by a period with failures.  \n",
    "\n",
    "and\n",
    "\n",
    "- $\\pi_0$ — Probability of having a failure on period \\(t\\), given that no failure occurred on period \\(t-1\\):  \n",
    "$$\n",
    "  \\pi_0 = \\frac{n01}{n00 + n01}\n",
    "$$\n",
    "\n",
    "- $\\pi_1$ — Probability of having a failure on period \\(t\\), given that a failure occurred on period \\(t-1\\):  \n",
    "  $$\n",
    "  \\pi_1 = \\frac{n11}{n10 + n11}\n",
    "  $$\n",
    "\n",
    "- $\\pi$ — Probability of having a failure on period \\(t\\):  \n",
    "  $$\n",
    "  \\pi = \\frac{n01 + n11}{n00 + n01 + n10 + n11}\n",
    "  $$\n",
    "\n",
    "\n",
    "This statistic is asymptotically distributed as a chi-square variable with 1 degree of freedom.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d00b051",
   "metadata": {},
   "source": [
    "### CC test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ba5a09",
   "metadata": {},
   "source": [
    "Christoffersen (1998) also proposed the **Conditional Coverage (CC) test**, which jointly evaluates whether the frequency of exceptions is correct (Unconditional Coverage, UC) **and** whether exceptions occur independently (Independence, IND).  \n",
    "\n",
    "The test statistic is simply the sum of the UC and IND likelihood ratio statistics:\n",
    "\n",
    "$$\n",
    "LR_{CC} = LR_{UC} + LR_{CCI}\n",
    "$$\n",
    "\n",
    "\n",
    "This statistic is asymptotically distributed as a chi-square variable with 2 degrees of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d18979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coverage_test_log_05(df_returns, df_VaR, alpha=0.05, test_alpha=0.05):\n",
    "    results_list = []\n",
    "    eps = 1e-12  # small epsilon to avoid log(0) / division by zero\n",
    "\n",
    "    for col_name in df_VaR.columns:\n",
    "        print(f\"\\nResults of {col_name}:\")\n",
    "        # Merge/align returns and VaR on the same date index.\n",
    "        df_test = \n",
    "        # Extract the aligned return series and VaR series\n",
    "        returns = \n",
    "        var_series = \n",
    "\n",
    "        #  Build the Exception indicator I_t = 1{ r_t < VaR_t } and drop NaN.\n",
    "        I = \n",
    "        I = \n",
    "        T = \n",
    "\n",
    "        # ---------- UC (Kupiec) ----------\n",
    "        # Count violations and non-violations\n",
    "        n_1 = \n",
    "        n_0 = \n",
    "        pi_hat_uc = \n",
    "\n",
    "        # Guard against log(0) by clipping probabilities with eps.\n",
    "        a0 = \n",
    "        a1 = \n",
    "        p0 = \n",
    "        p1 = \n",
    "        # UC likelihoods\n",
    "        log_L_uc_H0 = \n",
    "        log_L_uc_H1 = \n",
    "        # UC LR stat and p-value\n",
    "        LR_uc = \n",
    "        Pvalue_uc = \n",
    "\n",
    "        # ---------- Ind (Christoffersen independence) ----------\n",
    "        # Build lag/lead of I_t to count transitions n_00, n_01, n_10, n_11\n",
    "        I_lag = \n",
    "        I_lead = \n",
    "        n_00 = \n",
    "        n_01 = \n",
    "        n_10 = \n",
    "        n_11 = \n",
    "        \n",
    "        # Denominators and totals for transition probabilities.\n",
    "        denom0 = \n",
    "        denom1 = \n",
    "        total  = \n",
    "\n",
    "        # H0 overall failure prob and H1 transition probs\n",
    "        pi_hat_ind = \n",
    "        pi_hat_01 = \n",
    "        pi_hat_11 =\n",
    "\n",
    "        # Guard logs for Ind part\n",
    "        q0 = \n",
    "        q1 = \n",
    "        r00 = \n",
    "        r01 = \n",
    "        r10 = \n",
    "        r11 = \n",
    "        # Ind likelihoods\n",
    "        log_L_ind_H0 = \n",
    "        log_L_ind_H1 = \n",
    "        # Ind LR stat and p-value\n",
    "        LR_ind = \n",
    "        Pvalue_ind = \n",
    "\n",
    "        # ---------- CC (Christoffersen conditional coverage) ----------\n",
    "        \n",
    "        LR_cc = \n",
    "        Pvalue_cc = \n",
    "\n",
    "        print(f\"UC test:  LR = {LR_uc:.4f}, p-value = {Pvalue_uc:.4f}\")\n",
    "        print(f\"Ind test: LR = {LR_ind:.4f}, p-value = {Pvalue_ind:.4f}\")\n",
    "        print(f\"CC test:  LR = {LR_cc:.4f}, p-value = {Pvalue_cc:.4f}\")\n",
    "\n",
    "        results_list.append([col_name, Pvalue_uc, Pvalue_ind, Pvalue_cc])\n",
    "\n",
    "    df_results = pd.DataFrame(results_list, columns=[\"Company\", \"UC p-value\", \"Ind p-value\", \"CC p-value\"])\n",
    "\n",
    "    df_summary = pd.DataFrame({\n",
    "        f\"p < {test_alpha}\": [\n",
    "            np.sum(df_results[\"UC p-value\"]  < test_alpha),\n",
    "            np.sum(df_results[\"Ind p-value\"] < test_alpha),\n",
    "            np.sum(df_results[\"CC p-value\"]  < test_alpha)\n",
    "        ],\n",
    "        f\"p ≥ {test_alpha}\": [\n",
    "            np.sum(df_results[\"UC p-value\"]  >= test_alpha),\n",
    "            np.sum(df_results[\"Ind p-value\"] >= test_alpha),\n",
    "            np.sum(df_results[\"CC p-value\"]  >= test_alpha)\n",
    "        ],\n",
    "        \"NaN\": [\n",
    "            np.sum(df_results[\"UC p-value\"].isna()),\n",
    "            np.sum(df_results[\"Ind p-value\"].isna()),\n",
    "            np.sum(df_results[\"CC p-value\"].isna())\n",
    "        ]\n",
    "    }, index=[\"UC\", \"Ind\", \"CC\"])\n",
    "\n",
    "    df_reject = pd.DataFrame({\n",
    "        f\"Reject (<{test_alpha})\": [\n",
    "            df_results.loc[df_results[\"UC p-value\"]  < test_alpha, \"Company\"].tolist(),\n",
    "            df_results.loc[df_results[\"Ind p-value\"] < test_alpha, \"Company\"].tolist(),\n",
    "            df_results.loc[df_results[\"CC p-value\"]  < test_alpha, \"Company\"].tolist()\n",
    "        ]\n",
    "    }, index=[\"UC\", \"Ind\", \"CC\"])\n",
    "\n",
    "    return df_results, df_summary, df_reject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b99948",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_05, df_summary_05, df_reject_05 = coverage_test_log_05(df_ret_asset_bt, df_VaR_asset_bt, alpha=0.05, test_alpha = 0.05)\n",
    "print(df_summary_05)\n",
    "print(df_reject_05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70aa5bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd3d95ef",
   "metadata": {},
   "source": [
    "## CoVaR (Adrian and Brunnermeier, 2014)\n",
    "\n",
    "The **CoVaR** is the Value-at-Risk of the market return given a specific event on the firm return:\n",
    "\n",
    "$$\n",
    "\\Pr\\left( r_{mt} \\leq CoVaR_{t}^{m \\mid r_{it} = VaR_{it}(\\alpha)} \\ \\big|\\ r_{it} = VaR_{it}(\\alpha) \\right) = \\alpha.\n",
    "$$\n",
    "\n",
    "\n",
    "## ΔCoVaR\n",
    "\n",
    "The contribution of the institution to systemic risk, **ΔCoVaR**, is the difference between its CoVaR and the CoVaR calculated at the median state:\n",
    "\n",
    "$$\n",
    "\\Delta CoVaR_{it}(\\alpha) = CoVaR_{t}^{m \\mid r_{it} = VaR_{it}(\\alpha)} - CoVaR_{t}^{m \\mid r_{it} = Median(r_{it})}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Under the **Gaussian linear assumption**, the ΔCoVaR can be written as:\n",
    "\n",
    "$$\n",
    "\\Delta CoVaR_{\\alpha}^{m \\mid i,t+1} \n",
    "= \\rho_{m,i,t+1} \\times \\frac{\\sigma_{m,t+1}}{\\sigma_{i,t+1}} \\times \n",
    "\\Big( VaR_{\\alpha}^{i,t+1} - \\text{Median}(r_{i,t}) \\Big).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f74c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Market return\n",
    "df_ret_mkt = pd.read_csv('ret_mkt.csv')\n",
    "df_ret_mkt['Date'] = pd.to_datetime(df_ret_mkt['Date'])\n",
    "df_ret_mkt.set_index('Date', inplace = True)\n",
    "df_ret_mkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f68bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the market predicted volatility using function compute_var()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4766b232",
   "metadata": {},
   "source": [
    "# ρ in DCC(1,1): Engle (2002) Dynamic Conditional Correlation \n",
    "\n",
    "## 1) Covariance–correlation decomposition\n",
    "Let $H_t$ be the $N\\times N$ **conditional covariance** matrix of returns $\\varepsilon_t$ (innovations):\n",
    "$$\n",
    "H_t = D_t\\, R_t\\, D_t,\n",
    "$$\n",
    "where $D_t=\\mathrm{diag}\\big(\\sqrt{h_{1t}},\\ldots,\\sqrt{h_{Nt}}\\big)$ collects **univariate conditional volatilities** (from any GARCH for each series), and $R_t$ is the **time-varying correlation matrix**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Standardized residuals\n",
    "Define standardized residuals\n",
    "$$\n",
    "u_t = D_t^{-1}\\varepsilon_t = \\big(u_{1t},\\ldots,u_{Nt}\\big)^\\top, \n",
    "\\qquad u_{it}=\\frac{\\varepsilon_{it}}{\\sqrt{h_{it}}}.\n",
    "$$\n",
    "\n",
    "Let $\\bar Q=\\mathbb{E}[u_tu_t^\\top]$ denote the **unconditional covariance** of $u_t$ (in practice, the sample covariance/correlation of standardized residuals) and assume $\\bar Q$ is positive definite.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) DCC(1,1) dynamics (Engle, 2002)\n",
    "Introduce an auxiliary $N\\times N$ symmetric positive definite matrix $Q_t$ that follows a GARCH-type recursion:\n",
    "$$\n",
    "\\boxed{\\quad\n",
    "Q_t=(1-\\alpha-\\beta)\\,\\bar Q\\;+\\;\\alpha\\,u_{t-1}u_{t-1}^\\top\\;+\\;\\beta\\,Q_{t-1},\n",
    "\\quad \\alpha\\ge 0,\\;\\beta\\ge 0,\\;\\alpha+\\beta<1.\n",
    "\\quad}\n",
    "$$\n",
    "\n",
    "The **dynamic correlation matrix** is then obtained by scaling $Q_t$ to unit diagonal:\n",
    "$$\n",
    "\\boxed{\\quad\n",
    "R_t = \\mathrm{diag}(Q_t)^{-1/2}\\; Q_t\\; \\mathrm{diag}(Q_t)^{-1/2}.\n",
    "\\quad}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Pairwise correlation coefficient $ \\rho_{ij,t} $\n",
    "Let $q_{ij,t}$ denote the $(i,j)$ element of $Q_t$.  \n",
    "Then the DCC correlation between series $i$ and $j$ is\n",
    "$$\n",
    "\\boxed{\\quad\n",
    "\\rho_{ij,t}=\\frac{q_{ij,t}}{\\sqrt{q_{ii,t}\\,q_{jj,t}}}.\n",
    "\\quad}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Bivariate explicit formula \n",
    "For $N=2$, write $\\bar Q=\\begin{pmatrix}\\bar q_{11}&\\bar q_{12}\\\\ \\bar q_{12}&\\bar q_{22}\\end{pmatrix}$.\n",
    "The recursion for each element is\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q_{12,t} &= (1-\\alpha-\\beta)\\,\\bar q_{12} + \\alpha\\,u_{1,t-1}u_{2,t-1} + \\beta\\,q_{12,t-1},\\\\[2pt]\n",
    "q_{11,t} &= (1-\\alpha-\\beta)\\,\\bar q_{11} + \\alpha\\,u_{1,t-1}^2       + \\beta\\,q_{11,t-1},\\\\[2pt]\n",
    "q_{22,t} &= (1-\\alpha-\\beta)\\,\\bar q_{22} + \\alpha\\,u_{2,t-1}^2       + \\beta\\,q_{22,t-1}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "Hence the DCC(1,1) correlation is\n",
    "$$\n",
    "\\boxed{\\quad\n",
    "\\rho_{12,t}\n",
    "=\\frac{(1-\\alpha-\\beta)\\,\\bar q_{12}+\\alpha\\,u_{1,t-1}u_{2,t-1}+\\beta\\,q_{12,t-1}}\n",
    "{\\sqrt{\\Big((1-\\alpha-\\beta)\\,\\bar q_{11}+\\alpha\\,u_{1,t-1}^2+\\beta\\,q_{11,t-1}\\Big)\n",
    "      \\Big((1-\\alpha-\\beta)\\,\\bar q_{22}+\\alpha\\,u_{2,t-1}^2+\\beta\\,q_{22,t-1}\\Big)}}.\n",
    "\\quad}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69c2da6",
   "metadata": {},
   "source": [
    "The multivariate GARCH (MGARCH) in python is not well developped, so let's move to R just for illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f228567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Rho = pd.read_csv('DCC_Rho.csv')\n",
    "df_Rho['Date'] = pd.to_datetime(df_Rho['Date'])\n",
    "df_Rho.set_index('Date', inplace = True)\n",
    "df_Rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba218720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now calculate the Delta CoVaR as in the equation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ed5be4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
